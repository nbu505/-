# -*- coding: utf-8 -*-
import operator
import jieba
import jieba.analyse
import re
import math
import operator
import pymysql
import warnings
warnings.filterwarnings('ignore')

#----------------------------------Keyword---------------------------------------
class keyword(object):

    def __init__(self):
        self.stopwords = []
        self.keywords = []
        self.tf_idf_data = {}
        self.pre_data_map = {}
        self.num = 7000000#æ€»æ–‡æ¡£æ•°é‡
        self.tfidf = {}

    def daoru(self):
        with open('E:\è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†\classify\StopWordTable.txt',encoding='utf-8') as f:
            lines = f.readlines()
            for line in lines:
                line = line.replace('\n','')
                self.stopwords.append(line)

        with open(r'E:\è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†\classify\t_idf1.txt',encoding='utf-8') as f:
            lines = f.readlines()
            for line in lines:
                line = line.replace('\n','')
                line = line.split(',')
                self.tf_idf_data[line[1].strip("\"")]=int(line[2].strip("\""))

    def yuchuli(self,paper):
        r1 = '[a-zA-Z0-9â€™!"#$%&\'()*+,-./:;<=>?@ï¼Œã€‚?â˜…ã€â€¦ã€ã€‘ã€Šã€‹ï¼Ÿâ€œâ€â€˜â€™ï¼[\\]^_`{|}~]+'
        paper = re.sub(r1,'',paper)
        seg_list = jieba.lcut(paper)
        for word in seg_list:
            if len(word) > 1:
                if word not in self.stopwords:#å»åœç”¨è¯
                    if word in self.pre_data_map.keys():#å¯¹æ¯ä¸ªé•¿åº¦å¤§äºç­‰äº2çš„è¯è®°å½•å¹¶è®¡æ•°
                        self.pre_data_map[word] += 1
                    else:
                        self.pre_data_map[word] = 1

    def getTF(self,str):
        M = 0
        for s in self.pre_data_map:
            M = M + self.pre_data_map.get(s)
        return float(self.pre_data_map.get(str)/M)

    def getIDF(self,str):
        wendangNum = self.tf_idf_data.get(str)
        if wendangNum == None:
            wendangNum = 0
        res = float(math.log(float(self.num)/(wendangNum+1)))
        return res

    def getTFIDF(self,paper):
        self.daoru()
        self.yuchuli(paper)
        for str in self.pre_data_map.keys():
            self.tfidf[str] = self.getTF(str)*self.getIDF(str)
        self.tfidf=sorted(self.tfidf.items(),key=operator.itemgetter(1),reverse=True)
        num = int(len(self.tfidf)*0.22)
        # self.keywords = self.tfidf[:num]
        temp = self.tfidf[:num]
        for t in temp:
            self.keywords.append(t[0])#('é«˜èƒ½', 0.3810204035224749), ('äº¿é¢—', 0.3101682504706382)


# ------------------------------from zuclassify import zhClassify--------------------------------------
class zhClassify(object):

    def __init__(self):
        self.data_map = {}

    def daoru(self):
        with open('E:\è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†\classify\ztxkb4.txt',encoding='utf-8') as f:
            lines = f.readlines()
            for line in lines:
                line = line.replace('\n','')
                line = line.split(',')
                self.data_map[line[0].strip("\"")] = line[1].strip("\"")

    def data_process(self,ztNumber):
        # pattern = "[`~!@#$%^&*()--+=|{}':;',//[//]<>/?~ï¼@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰â€”â€”+|{}ã€ã€‘â€˜ï¼›ï¼šâ€â€œâ€™ã€‚ï¼Œã€ï¼Ÿ]"
        # ztNumber = re.sub(pattern,'',ztNumber)
        # self.daoru()

        zt = ztNumber.split(';')

        res = ''

        for sn in zt:
            sn_fir = sn
            if len(sn) > 2:
                if sn[0:3] == "S78" or sn[0:3] == "S88" or sn[0:3] == "S89" or sn[0:3]=="U67" :
                    sn = sn_fir
                elif sn[0] == "E":
                    sn = "E"
                elif sn[0:3]=="G25":
                    sn = "G250"
                elif sn[0:5]=="TS941" or sn[0:5]=="TS942" or sn[0:4]=="TS94" or sn[0:3]=="TS1":
                    sn = "TS1"
                elif sn[0:3]=="F23" or sn[0:3]=="F206" or sn[0:3]=="F203" or sn[0:4]=="F719" or sn[0:4]=="F718"or sn[0:4]=="F717"or sn[0:4]=="F716"or sn[0:4]=="F715"or sn[0:4]=="F714"or sn[0:4]=="F593" or sn[0:4]=="F715" or sn[0:5]=="F590."or sn[0:4]=="F592"or sn[0:4]=="F594" or sn[0:4]=="F595" or sn[0:4]=="F596" or sn[0:4]=="F597":
                    sn = "F72"
                else:
                    sn = sn[0:3]
            else:
                if sn[0:2] == "R1" or sn[0:2] == "S3" or sn[0:2] == "S9":
                    sn = sn_fir
                else:
                    sn = sn[0:2]

            try:
                res_ch = self.data_map[sn]
            except KeyError:
                res_ch=''
            if sn_fir != zt[-1] and res_ch!='':
                res = res + res_ch +';'
            else:
                res = res + res_ch
        return res

#-------------------------------------keyword_extract__________________________________________
class Extract(object):
    def keyword_extract(self,input):
        # regul = 'a-zA-Z0-9'
        regul = '0-9'
        jieba.analyse.set_stop_words('E:\è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†\classify\StopWordTable.txt')
        content = re.sub(regul, '', input)
        keywords = jieba.analyse.extract_tags(content, topK=4, withWeight=False, allowPOS=())
        return keywords

#---------------------------------from BayesClassify import Classify---------------------------------
class Classify(object):

    def __init__(self):
        # sw = keyword()
        # sw.yuchuli('æ­£ä¸­å›½ç§‘å­¦é™¢2017å¹´11æœˆ29æ—¥ä¸¾è¡Œæ–°é—»å‘å¸ƒä¼š,å®£å¸ƒä¸­ç§‘é™¢ç©ºé—´ç§‘å­¦æˆ˜ç•¥æ€§å…ˆå¯¼ä¸“é¡¹é¦–å‘æ˜Ÿâ€”â€”æš—ç‰©è´¨ç²’å­æ¢æµ‹å«æ˜Ÿâ€œæ‚Ÿç©ºâ€(DAMPE)å–å¾—é¦–æ‰¹é‡å¤§ç§‘å­¦æˆæœã€‚åˆ©ç”¨DAMPEé‡‡é›†åˆ°çš„æ•°æ®è·å¾—äº†è¿„ä»Šæœ€ç²¾ç¡®çš„é«˜èƒ½ç”µå­å®‡å®™çº¿èƒ½è°±ã€‚ç›¸å…³æˆæœäº2017å¹´11æœˆ30æ—¥æ­£å¼åœ¨Natureåœ¨çº¿å‘è¡¨ã€‚DAMPEäº2015å¹´12æœˆ17æ—¥å‘å°„æˆåŠŸ,åœ¨è½¨è¿è¡Œå‰530å¤©å…±é‡‡é›†çº¦28äº¿é¢—é«˜èƒ½å®‡å®™å°„çº¿,å…¶ä¸­åŒ…')
        # sw.getTFIDF('æ³•å›½ç”µå½±ä½œä¸ºç”µå½±ç•Œçš„å…ƒè€,ä»¥å…¶æ·±åšçš„æ–‡åŒ–åº•è•´ä¸ç‹¬ç‰¹çš„è‰ºæœ¯æ™ºæ…§,è´¨æœ´è€ŒçŠ€åˆ©åœ°å±•ç°å‡ºäººä¸è‡ªç„¶ä¹‹é—´ã€äººä¸äººä¹‹é—´ã€äººä¸è‡ªæˆ‘ä¹‹é—´çš„çŸ›ç›¾,æŠ˜å°„å‡ºäººç±»åŸæ¬²çš„æƒ…æ„«,å¹¶å¾ªå¾ªå–„è¯±äºˆè§‚å½±äººä»¥å¯è¿ªã€‚æœ¬æ–‡é€šè¿‡å¯¹æ³•å›½ç”µå½±ã€Šè´è¶ã€‹çš„å‰–æ,æ—¨åœ¨åˆ†ææ³•å›½ç”µå½±çš„ç²¾ç¥æ¬²æœ›æ„å»ºä»¥åŠå…¶å¯¹ä¿ƒè¿›å»ºè®¾å’Œè°äººç±»ç²¾ç¥ç”Ÿæ€çš„å½±å“,è‚¯å®šäº†æ³•å›½ç”µå½±çš„é‡è¦åœ°ä½,å¹¶å€Ÿä»¥ä¼ è¾¾å‡ºç”µå½±åœ¨äºˆäººæ¶ˆé£çš„åŒæ—¶,åº”ä¸å¿˜è‰ºæœ¯æ‰€æ‰¿è½½çš„ç¤¾ä¼šä½¿å‘½çš„è´£ä»»ã€‚')
        self.M = 14892.0
        self.Num = 1278586 #åˆ†ç±»å™¨è®­ç»ƒè¯­æ–™æ•°é‡
        # self.wordList = sw.keywords
        self.wordList = []
        self.bayes1 = {}
        self.bayes2 = {}
        self.hs = {}

    def daorudata(self):
        with open(r'E:\è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†\classify\t_bayespro1.txt',encoding='utf-8') as f:
            lines = f.readlines()
            for line in lines:
                line = line.replace('\n', '')
                line = line.split(',')
                self.bayes1[line[2].strip("\"")] = int(line[3].strip("\""))

        with open(r'E:\è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†\classify\t_bayespro22.txt',encoding='utf-8') as f:
            lines = f.readlines()
            for line in lines:
                line = line.replace('\n','')
                line = line.split(',')
                self.bayes2[(line[1].strip("\""))+'-_-'+(line[2].strip("\""))] = int(line[3].strip("\""))


    def calculatePc(self,cj):  #è®¡ç®—pï¼ˆcjï¼‰
        a = float(self.bayes1.get(cj))
        return float(a/self.Num)

    def getTrainingNumOfClassification(self,cj): #å¾—åˆ°cjçš„è¯­æ–™æ•°é‡
        return float(self.bayes1.get(cj))

    def getCountContainKeyOfClassification(self,cj,ss):#å¾—åˆ°numï¼ˆxi|cjï¼‰
        ret = 0.0
        try:
            ret = float(self.bayes2.get(ss+'-_-'+cj))
        except:
            pass
        return ret

    def pxc(self,ss,cj): #è®¡ç®—pï¼ˆxi|cjï¼‰
        Nxc = self.getCountContainKeyOfClassification(cj,ss)
        Nc = self.getTrainingNumOfClassification(cj)
        ret = float((Nxc+1)/(Nc+self.M))
        return ret

    def calcProd(self,cj):#è®¡ç®—ğ‘(ğ‘¥_1,ğ‘¥_2,â€¦,ğ‘¥_ğ‘›â”‚ğ‘_ğ‘— )
        ret = 1.0
        for ss in self.wordList:
            ret = ret * self.pxc(ss,cj) * 1000
        ret = ret * self.calculatePc(cj)
        # print(ret)
        return ret

    def classfy(self,inputContent):
        # sw = keyword()
        # # sw.yuchuli('æ­£ä¸­å›½ç§‘å­¦é™¢2017å¹´11æœˆ29æ—¥ä¸¾è¡Œæ–°é—»å‘å¸ƒä¼š,å®£å¸ƒä¸­ç§‘é™¢ç©ºé—´ç§‘å­¦æˆ˜ç•¥æ€§å…ˆå¯¼ä¸“é¡¹é¦–å‘æ˜Ÿâ€”â€”æš—ç‰©è´¨ç²’å­æ¢æµ‹å«æ˜Ÿâ€œæ‚Ÿç©ºâ€(DAMPE)å–å¾—é¦–æ‰¹é‡å¤§ç§‘å­¦æˆæœã€‚åˆ©ç”¨DAMPEé‡‡é›†åˆ°çš„æ•°æ®è·å¾—äº†è¿„ä»Šæœ€ç²¾ç¡®çš„é«˜èƒ½ç”µå­å®‡å®™çº¿èƒ½è°±ã€‚ç›¸å…³æˆæœäº2017å¹´11æœˆ30æ—¥æ­£å¼åœ¨Natureåœ¨çº¿å‘è¡¨ã€‚DAMPEäº2015å¹´12æœˆ17æ—¥å‘å°„æˆåŠŸ,åœ¨è½¨è¿è¡Œå‰530å¤©å…±é‡‡é›†çº¦28äº¿é¢—é«˜èƒ½å®‡å®™å°„çº¿,å…¶ä¸­åŒ…')
        # sw.getTFIDF(inputContent)
        # self.wordList = sw.keywords

        extract = Extract()
        self.wordList=extract.keyword_extract(inputContent)

        self.daorudata()
        probility = 0.0
        for bc1 in self.bayes1.keys():
            probility = self.calcProd(bc1)
            self.hs[bc1] = probility
        sh = sorted(self.hs.items(),key = operator.itemgetter(1),reverse=True)
        return sh[0][0]



#----------------------------------process------------------------------------
cn_map_id = {}
with open('E:\è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†\classify\dictionaries_category.csv',encoding='utf-8') as f:
    lines = f.readlines()
    for line in lines:
        line = line.replace('\n', '')
        line = line.split(',')
        cn_map_id[line[1]]=line[0]


def processing(ztNumber,content):

    ztfenlei = zhClassify()
    ztfenlei.daoru()
    beiyesi = Classify()

    res = ''
    if ztNumber is not None:
        res = ztfenlei.data_process(ztNumber)
        if res !='':
            # print(res)
            print("***********ä¸­å›¾å·åˆ†ç±»************")
        else:
            res = beiyesi.classfy(content)


    res_final_id = ''
    resl = res.split(';')
    length = len(resl)
    tem = resl[0]
    for i in range(length):
        # print(i)
        res_id = cn_map_id[resl[i]]
        if i>=1:
            if tem == resl[i]:
                res_final_id =  res_id
                break

        if i == length-1:
            res_final_id = res_final_id + res_id
        else:
            res_final_id = res_final_id + res_id +';'

    return res,res_final_id

#-------------------------------connectMysql-------------------------------------
db = pymysql.connect("localhost","root","nbu505","xstt_local_testdb")
cursor = db.cursor()
cursor2 = db.cursor()

sql = 'select `content`,`title`,`category`,`id`  from journals_cleaned where id >0'
sql2 = "update journals_cleaned set label = ('%s'),category_id = ('%s'),category_cn = ('%s') where id = ('%d')"
cursor.execute(sql)

db.commit()

# print(cursor.fetchmany(10))
result = 1
# list1 = []
# list2 = []
# list3 = []
# s =10
extract = Extract()
while result:
    # s=s-1
    result = cursor.fetchone()
    if not result:
        break
    content = ''.join(result[0])
    #print(content)
    label_str = ''
    label = extract.keyword_extract(content)
    for l in label[:-1]:
        label_str = label_str + l +','
    label_str += label[-1]
    cate,cate_id  = processing(result[2],result[0]+result[1])
    sql_id = int(result[3])

    data = (label_str,cate_id,cate,sql_id)
    print(data)
    cursor2.execute(sql2%data)
    db.commit()

cursor.close()
cursor2.close()
db.close()